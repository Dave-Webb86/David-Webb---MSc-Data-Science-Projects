{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Component Four David Webb 20104099 <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libraries imported\n"
     ]
    }
   ],
   "source": [
    "# import of necessary libraries for the data manipulation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import dummy\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "print('libraries imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data imported\n"
     ]
    }
   ],
   "source": [
    "# load master dataset\n",
    "df = pd.read_csv('df.csv')\n",
    "\n",
    "print('data imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simplify the ratings variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge not good ratings into one output\n",
    "ng = ['Inadequate', 'Requires Improvement']\n",
    "\n",
    "for x in ng:\n",
    "    df.loc[df['Rating'].str.contains(x), 'Rating'] = 'Not Good'\n",
    "\n",
    "# drop unknown rating data from the dataset\n",
    "df = df[df['Rating'] != 'Unknown']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Establish baseline for a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Good           1625\n",
       "Not Good        612\n",
       "Outstanding     506\n",
       "Name: Rating, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check frequencies of the target variable possibilities\n",
    "df['Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy for the 'Rating' target variable: 59.2%\n"
     ]
    }
   ],
   "source": [
    "# establish baseline accuracy\n",
    "\n",
    "# set up dummy classifier model\n",
    "# classifier is chosen due to non-numeric target variable\n",
    "cls = dummy.DummyClassifier(strategy = 'most_frequent')\n",
    "X = df.drop('Rating', axis=1)\n",
    "y = df['Rating']\n",
    "cls.fit(X, y)\n",
    "y_pred = cls.predict(X)\n",
    "\n",
    "# report dummy classifier accuracy\n",
    "baseline = round(metrics.accuracy_score(y, y_pred), 3)\n",
    "print(\"Baseline accuracy for the 'Rating' target variable: \" + str(round(baseline * 100, 1)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline accuracy has been calculated using a most frequent strategy, where the model has simply predicted that target is always the most frequent value from the raw data.  This strategy was chosen as upon assessing the raw data the 'good' outcome is ~3x more likely to be chosen than either of the other two potential outcomes, rather than an approximately even split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run models with no feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set features to float only values to avoid errors in classifiers\n",
    "X = df.drop(['LA', 'GENDER', 'RELCHAR', 'PPERSABS10', 'P8_BANDING', 'OVERALL_DESTPER', 'NOT_SUSTAINEDPER', 'UNKNOWNPER',\n",
    "       'OVERALL_DESTPER_DIS', 'NOT_SUSTAINEDPER_DIS', 'UNKNOWNPER_DIS', 'Rating'], axis=1)\n",
    "y = df['Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.191"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run k-NN model on data using default setting (k=5, uniform weighting and euclidean distance)\n",
    "from sklearn import neighbors\n",
    "clf = neighbors.KNeighborsClassifier()\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(clf, X, y, cv=5, scoring = 'accuracy')\n",
    "knn = round(scores.mean(),3)\n",
    "knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.573"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run GaussianNB model on data\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "scores = cross_val_score(clf, X, y, cv=5, scoring = 'accuracy')\n",
    "gausNB = round(scores.mean(),3)\n",
    "gausNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.579"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run BernoulliNB model on data\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "scores = cross_val_score(clf, X, y, cv=5, scoring = 'accuracy')\n",
    "bernNB = round(scores.mean(),3)\n",
    "bernNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.627"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run multiclass logistic regression model on the data\n",
    "# a 'onevrest' strategy is chosen for the multiclass method\n",
    "from sklearn import linear_model\n",
    "from sklearn import multiclass\n",
    "logreg = multiclass.OneVsRestClassifier(linear_model.LogisticRegression(solver='lbfgs', max_iter=10000))\n",
    "scores = cross_val_score(logreg, X, y, cv=5, scoring = 'accuracy')\n",
    "log_reg = round(scores.mean(),3)\n",
    "log_reg\n",
    "\n",
    "# logistic regression classifier set to the default 'lbfgs' solver with a maximum number of 1000 iterations.\n",
    "# this is to stop warning errors for setting the default solver type and non-convergence timeouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.417"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removal of varaibles with negative data to allow MultinomialNB to be run on the data\n",
    "X1 = X.drop(['P8MEA', 'P8CILOW', 'P8CIUPP'], axis=1)\n",
    "\n",
    "# run MultinomialNB model on data\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "scores = cross_val_score(clf, X1, y, cv=5, scoring = 'accuracy')\n",
    "multiNB = round(scores.mean(),3)\n",
    "multiNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>No Feature Engineering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>k-NN</td>\n",
       "      <td>0.191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Classifier  No Feature Engineering\n",
       "0             Baseline                   0.592\n",
       "1  Logistic Regression                   0.627\n",
       "2                 k-NN                   0.191\n",
       "3        MultinomialNB                   0.417\n",
       "4           GaussianNB                   0.573\n",
       "5          BernoulliNB                   0.579"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tabulate model data for easy comparison\n",
    "acc_scores = pd.DataFrame({'Classifier':['Baseline', 'Logistic Regression', 'k-NN', 'MultinomialNB', 'GaussianNB', 'BernoulliNB'],\n",
    "    'No Feature Engineering':[baseline, log_reg, knn, multiNB, gausNB, bernNB]})\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no feature engineering a multiclass logistic regression classifier shows the best accuracy on the data in predicting a schools OFFSTED rating, with an accuracy of 62.7% (compared to a baseline accuracy of 59.2%).  Interestingly all the other classifiers tested using this method produce worse results than the baseline accuracy of 59.2%, with k-NN performing particularly poorly. <br><br>\n",
    "It is also noted that the GaussianNB method would be the most sensible Naive Bayes classifier for the data as the majority of the variables are continuous, but all Naive Bayes classifiers are tested for completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it provided the best accuracy the mutliclass logistic regression classifier was chosen to generate a confusion matrix of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good      0.642     0.937     0.762      1625\n",
      "    Not Good      0.674     0.155     0.252       612\n",
      " Outstanding      0.728     0.328     0.452       506\n",
      "\n",
      "    accuracy                          0.650      2743\n",
      "   macro avg      0.681     0.474     0.489      2743\n",
      "weighted avg      0.665     0.650     0.591      2743\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run the logistic regression classifier across all the data to generate predicted results\n",
    "logreg = multiclass.OneVsRestClassifier(linear_model.LogisticRegression(solver='lbfgs', max_iter=10000))\n",
    "logreg.fit(X, y)\n",
    "y_pred = logreg.predict(X)\n",
    "\n",
    "\n",
    "# generate a confusion matrix of the predicted results\n",
    "ratings = ['Outstanding','Good','Not Good']\n",
    "cm = metrics.confusion_matrix(y_pred, y, labels=ratings)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm', ax=ax, cbar=False)\n",
    "ax.set_xticklabels(ratings)\n",
    "ax.set_yticklabels(ratings)\n",
    "ax.set_xlabel('Predicted Rating')\n",
    "ax.set_ylabel('True Rating')\n",
    "\n",
    "# generate classification metrics for the data\n",
    "print(metrics.classification_report(y, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the logistic regression classifier over the whole shows a slightly improved accuracy score to the earlier calculated cross-validated score (65.0% v 62.7%) respectively.  A high amount of the confusion appears to be coming from the misclassification of good schools into the other two categories, which is perhaps understandable as the overwhelming number of schools assessed are classified as good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Feature Engineering - Progress 8 Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Progress 8 features of the dataset have negative values indicating a less than average improvement for the average student within a school.  This causes issues as certain ML classifiers cannot accept negative values (such as Multinomial Naive Bayes).  A simple solve for this is to shift the axis so all values are positive.<br><br>\n",
    "The axis is to be shifted for all the progress 8 variable by +5 to allow them all to still be comparable and convert all their values to positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['P8MEA_shifted'] = df['P8MEA'] + 5\n",
    "df['P8CILOW_shifted'] = df['P8CILOW'] + 5\n",
    "df['P8CIUPP_shifted'] = df['P8CIUPP'] + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Feature Engineering - GENDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two potential methods for converting the nominal 'GENDER' variable into a usable ML format:\n",
    "- Use one hot encoding to generate binomial columns for each of the 3 optional features<br><br>\n",
    "- Combine the sparce 'boys' and 'girls' classes into one, before converting the variable into a binomial 'is_single_sex' variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option taken to combine the sparce classes and convert into one column as the occurrence of single sex schools is so remote compared to mixed sex establishments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new varaible to identify if a school is single sex\n",
    "df['is_same_sex'] = df['GENDER'].map({'Mixed':0, 'Girls':1, 'Boys':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Feature Engineering - RELCHAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the occurrences of religious affiliated schools are so sparce compared to the occurrences of schools with no religious affiliation we are justified to combine all the different religious schools into one 'is_RELCHAR' variable to simplify the data and render it usable for ML.  Also if all the different religions are included in the data this may add a lot of noise with additional features that do not contain large amounts of data as machine learning algorithms mostly work under the assumption that categorical data is uniformly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2285\n",
       "1     458\n",
       "Name: is_RELCHAR, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new varaible to identify if a school has a religeous affiliation\n",
    "df['is_RELCHAR'] = np.where(df['RELCHAR'] == 'None', 0, 1)\n",
    "\n",
    "# check value counts of the new varaible\n",
    "df['is_RELCHAR'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Re-run of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>No Feature Engineering</th>\n",
       "      <th>Feature Engineering 1st Pass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>k-NN</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Classifier  No Feature Engineering  Feature Engineering 1st Pass\n",
       "0             Baseline                   0.592                         0.592\n",
       "1  Logistic Regression                   0.627                         0.643\n",
       "2                 k-NN                   0.191                         0.191\n",
       "3        MultinomialNB                   0.417                         0.420\n",
       "4           GaussianNB                   0.573                         0.573\n",
       "5          BernoulliNB                   0.579                         0.571"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label the model run for the table\n",
    "label = 'Feature Engineering 1st Pass'\n",
    "\n",
    "# define new feature columns\n",
    "X = df.drop(['LA', 'GENDER', 'RELCHAR', 'PPERSABS10', 'P8_BANDING', 'OVERALL_DESTPER', 'NOT_SUSTAINEDPER', 'UNKNOWNPER',\n",
    "       'OVERALL_DESTPER_DIS', 'NOT_SUSTAINEDPER_DIS', 'UNKNOWNPER_DIS', 'Rating', 'P8MEA', 'P8CILOW', 'P8CIUPP'], axis=1)\n",
    "y = df['Rating']\n",
    "\n",
    "classifiers = [multiclass.OneVsRestClassifier(linear_model.LogisticRegression(solver='lbfgs', max_iter=10000)), \n",
    "               neighbors.KNeighborsClassifier(),\n",
    "               MultinomialNB(), GaussianNB(), BernoulliNB()]\n",
    "\n",
    "# run series of ML classifiers to assess accuracy\n",
    "acc = [baseline]\n",
    "\n",
    "for clf in classifiers:\n",
    "    scores = cross_val_score(clf, X, y, cv=5, scoring = 'accuracy')\n",
    "    acc.append(round(scores.mean(), 3))\n",
    "    \n",
    "# append new accuracy scores to table for comparison\n",
    "acc_scores[label] = acc\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Largest positive change from the 1st pass of feature engineering is seen in the logistic regression classifier which has improved in accuracy by 1.6%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Feature Engineering - Mixed Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the mixed feature columns the decision was taken to replace the suppressed values with the average for the variable.  This decision was taken because often the values are suppressed as the number of pupils referred to is less than 5, such small populations can often provide outliers in data so by replacing with the average any outlier effects should be negated. <br><br>\n",
    "The data interpolation was done as follows:\n",
    " - The 'PPERSABS10', 'OVERALL_DESTPER', 'NOT_SUSTAINEDPER', 'OVERALL_DESTPER_DIS', and 'NOT_SUSTAINEDPER_DIS' variable column suppressed values were replaced with the median average value from the data (median was chosen as the more robust centrality measure due to the heavy skewing of the data in the variables) <br><br>\n",
    " - 'UNKNOWNPER' was re-calculated as the remainder from the 'OVERALL_DESTPER' and 'NOT_SUSTAINEDPER' variables to ensure the populations added up to 100% <br><br>\n",
    " - 'UNKNOWNPER_DIS' was re-calculated as the remainder from the 'OVERALL_DESTPER_DIS' and 'NOT_SUSTAINEDPER_DIS' variables to ensure the populations added up to 100% <br><br>\n",
    " - 'P8_BANDING' was shown to be related to a schools 'P8MEA' score in previous analysis (component 2 - section 7).  To replace suppressed values the mean 'P8MEA' score for each 'P8_BANDING' is calculated for available data, and the suppressed values then replaced by the banding of the closest banding mean 'P8MEA' to their own score <br><br>\n",
    " - All columns are given the suffix 'NO_SUPP' to show that they have been modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new badning varaible with no suppressed values\n",
    "df['P8_BANDING_NO_SUPP'] = df['P8_BANDING']\n",
    "\n",
    "# create banding boundaries\n",
    "B1 = (df.groupby('P8_BANDING')['P8MEA'].mean()[0] + df.groupby('P8_BANDING')['P8MEA'].mean()[1]) / 2\n",
    "B2 = (df.groupby('P8_BANDING')['P8MEA'].mean()[1] + df.groupby('P8_BANDING')['P8MEA'].mean()[2]) / 2\n",
    "B3 = (df.groupby('P8_BANDING')['P8MEA'].mean()[2] + df.groupby('P8_BANDING')['P8MEA'].mean()[3]) / 2\n",
    "B4 = (df.groupby('P8_BANDING')['P8MEA'].mean()[3] + df.groupby('P8_BANDING')['P8MEA'].mean()[4]) / 2\n",
    "\n",
    "# set conditions to define banding\n",
    "c1 = (df['P8MEA'] >= B1) & (df['P8_BANDING'] == 'SUPP')\n",
    "c2 = (df['P8MEA'] >= B2) & (df['P8MEA'] < B1) & (df['P8_BANDING'] == 'SUPP')\n",
    "c3 = (df['P8MEA'] >= B3) & (df['P8MEA'] < B2) & (df['P8_BANDING'] == 'SUPP')\n",
    "c4 = (df['P8MEA'] >= B4) & (df['P8MEA'] < B3) & (df['P8_BANDING'] == 'SUPP')\n",
    "c5 = (df['P8MEA'] < B4) & (df['P8_BANDING'] == 'SUPP')\n",
    "\n",
    "# replace 'SUPP' values with calculated banding\n",
    "df.loc[c1,'P8_BANDING_NO_SUPP'] = 1\n",
    "df.loc[c2,'P8_BANDING_NO_SUPP'] = 2\n",
    "df.loc[c3,'P8_BANDING_NO_SUPP'] = 3\n",
    "df.loc[c4,'P8_BANDING_NO_SUPP'] = 4\n",
    "df.loc[c5,'P8_BANDING_NO_SUPP'] = 5\n",
    "\n",
    "# reset new varaible to integer values\n",
    "df['P8_BANDING_NO_SUPP'] = df['P8_BANDING_NO_SUPP'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replaced suppressed values with the median from the variable\n",
    "df['PPERSABS10_NO_SUPP'] = df['PPERSABS10'].replace(['SUPP','NE','LOWCOV', 'SP'], \n",
    "                                                    df['PPERSABS10'].replace(['SUPP','NE','LOWCOV', 'SP'], np.nan).median())\n",
    "\n",
    "df['OVERALL_DESTPER_NO_SUPP'] = df['OVERALL_DESTPER'].replace(['SUPP','NE','LOWCOV', 'SP'], \n",
    "                                                    df['OVERALL_DESTPER'].replace(['SUPP','NE','LOWCOV', 'SP'], np.nan).median())\n",
    "\n",
    "df['NOT_SUSTAINEDPER_NO_SUPP'] = df['NOT_SUSTAINEDPER'].replace(['SUPP','NE','LOWCOV', 'SP'], \n",
    "                                                    df['NOT_SUSTAINEDPER'].replace(['SUPP','NE','LOWCOV', 'SP'], np.nan).median())\n",
    "\n",
    "df['OVERALL_DESTPER_DIS_NO_SUPP'] = df['OVERALL_DESTPER_DIS'].replace(['SUPP','NE','LOWCOV', 'SP'], \n",
    "                                                    df['OVERALL_DESTPER_DIS'].replace(['SUPP','NE','LOWCOV', 'SP'], np.nan).median())\n",
    "\n",
    "df['NOT_SUSTAINEDPER_DIS_NO_SUPP'] = df['NOT_SUSTAINEDPER_DIS'].replace(['SUPP','NE','LOWCOV', 'SP'], \n",
    "                                                    df['NOT_SUSTAINEDPER_DIS'].replace(['SUPP','NE','LOWCOV', 'SP'], np.nan).median())\n",
    "\n",
    "# reset varaibles to float values\n",
    "new_var = ['PPERSABS10_NO_SUPP', 'OVERALL_DESTPER_NO_SUPP', 'NOT_SUSTAINEDPER_NO_SUPP', \n",
    "   'OVERALL_DESTPER_DIS_NO_SUPP', 'NOT_SUSTAINEDPER_DIS_NO_SUPP']\n",
    "\n",
    "df[new_var] = df[new_var].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate remaining values for the 'UNKNOWNPER' and 'UNKNOWNPER_DIS' varaibles\n",
    "df['UNKNOWNPER_NO_SUPP'] = 1 - (df['OVERALL_DESTPER_NO_SUPP'] + df['NOT_SUSTAINEDPER_NO_SUPP'])\n",
    "\n",
    "df['UNKNOWNPER_DIS_NO_SUPP'] = 1 - (df['OVERALL_DESTPER_DIS_NO_SUPP'] + df['NOT_SUSTAINEDPER_DIS_NO_SUPP'])\n",
    "\n",
    "# resetting any negative values in the columns to 0 for sanity purposes\n",
    "df[['UNKNOWNPER_NO_SUPP','UNKNOWNPER_DIS_NO_SUPP']] = np.where(df[['UNKNOWNPER_NO_SUPP','UNKNOWNPER_DIS_NO_SUPP']] < 0,\n",
    "                                                              0, df[['UNKNOWNPER_NO_SUPP','UNKNOWNPER_DIS_NO_SUPP']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Re-run of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>No Feature Engineering</th>\n",
       "      <th>Feature Engineering 1st Pass</th>\n",
       "      <th>Feature Engineering 2nd Pass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>k-NN</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Classifier  No Feature Engineering  Feature Engineering 1st Pass  \\\n",
       "0             Baseline                   0.592                         0.592   \n",
       "1  Logistic Regression                   0.627                         0.643   \n",
       "2                 k-NN                   0.191                         0.191   \n",
       "3        MultinomialNB                   0.417                         0.420   \n",
       "4           GaussianNB                   0.573                         0.573   \n",
       "5          BernoulliNB                   0.579                         0.571   \n",
       "\n",
       "   Feature Engineering 2nd Pass  \n",
       "0                         0.592  \n",
       "1                         0.647  \n",
       "2                         0.191  \n",
       "3                         0.435  \n",
       "4                         0.595  \n",
       "5                         0.579  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label the model run for the table\n",
    "label = 'Feature Engineering 2nd Pass'\n",
    "\n",
    "# define new feature columns\n",
    "X = df.drop(['LA', 'GENDER', 'RELCHAR', 'PPERSABS10', 'P8_BANDING', 'OVERALL_DESTPER', 'NOT_SUSTAINEDPER', 'UNKNOWNPER',\n",
    "       'OVERALL_DESTPER_DIS', 'NOT_SUSTAINEDPER_DIS', 'UNKNOWNPER_DIS', 'Rating', 'P8MEA', 'P8CILOW', 'P8CIUPP'], axis=1)\n",
    "y = df['Rating']\n",
    "\n",
    "# run series of ML classifiers to assess accuracy\n",
    "acc = [baseline]\n",
    "\n",
    "for clf in classifiers:\n",
    "    scores = cross_val_score(clf, X, y, cv=5, scoring = 'accuracy')\n",
    "    acc.append(round(scores.mean(), 3))\n",
    "    \n",
    "# append new accuracy scores to table for comparison\n",
    "acc_scores[label] = acc\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2nd pass of feature engineering has seen the largest positive impact of the Naive Bayes classifiers, with the Multinomial and Gaussian methods increasing in accuracy by 1.5 and 2.2% respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Normalising Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>No Feature Engineering</th>\n",
       "      <th>Feature Engineering 1st Pass</th>\n",
       "      <th>Feature Engineering 2nd Pass</th>\n",
       "      <th>Normalised Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>k-NN</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Classifier  No Feature Engineering  Feature Engineering 1st Pass  \\\n",
       "0             Baseline                   0.592                         0.592   \n",
       "1  Logistic Regression                   0.627                         0.643   \n",
       "2                 k-NN                   0.191                         0.191   \n",
       "3        MultinomialNB                   0.417                         0.420   \n",
       "4           GaussianNB                   0.573                         0.573   \n",
       "5          BernoulliNB                   0.579                         0.571   \n",
       "\n",
       "   Feature Engineering 2nd Pass  Normalised Features  \n",
       "0                         0.592                0.592  \n",
       "1                         0.647                0.665  \n",
       "2                         0.191                0.628  \n",
       "3                         0.435                0.604  \n",
       "4                         0.595                0.581  \n",
       "5                         0.579                0.600  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label the model run for the table\n",
    "label = 'Normalised Features'\n",
    "\n",
    "# define new feature columns\n",
    "X = df.drop(['LA', 'GENDER', 'RELCHAR', 'PPERSABS10', 'P8_BANDING', 'OVERALL_DESTPER', 'NOT_SUSTAINEDPER', 'UNKNOWNPER',\n",
    "       'OVERALL_DESTPER_DIS', 'NOT_SUSTAINEDPER_DIS', 'UNKNOWNPER_DIS', 'Rating', 'P8MEA', 'P8CILOW', 'P8CIUPP'], axis=1)\n",
    "# import normalisation and apply to features\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "y = df['Rating']\n",
    "\n",
    "# run series of ML classifiers to assess accuracy\n",
    "acc = [baseline]\n",
    "\n",
    "for clf in classifiers:\n",
    "    scores = cross_val_score(clf, X, y, cv=5, scoring = 'accuracy')\n",
    "    acc.append(round(scores.mean(), 3))\n",
    "    \n",
    "# append new accuracy scores to table for comparison\n",
    "acc_scores[label] = acc\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisation of the features has greatly improved the accuracy of the k-NN classifier, jumping from 19.1 to 62.8%, indicating that the differences in scaling of some of the features was having a large impact on its performance. <br><br>\n",
    "We have also seen a large increase in the accuracy of the MultinomialNB classifier (by 16.9%) and more modest but significant increases in the accuracy of the logistic regression and BernouilliNB classifiers (1.8 and 2.1% respectively).<br><br>\n",
    "Interestingly the performance of the GaussianNB classifier decreased with normalisation of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Further Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further future engineering techniques tried as follows:\n",
    "1.\tConverting heavily skewed data to a more uniform distribution\n",
    "    -\tThis is done as we have a few features in the dataset which are heavily skewed and it will dampen the influence of outliers in the data\n",
    "<br><br>\n",
    "2.\tGenerating polynomial features in the data\n",
    "    -\tThis is done to assess whether there are any feature interactions which are useful for the classifier as these would not be captured if using the features individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Converting data to more uniform distributions\n",
    "\n",
    "# select heavily skewed data to convert\n",
    "columns = ['PSENELK', 'PFSM', 'PNUMFSMEVER', 'PTEBACC_94', 'PTEBACC_95', 'PPERSABS10_NO_SUPP']\n",
    "\n",
    "# convert 'PTEBACC' varaible to 100% to avoid issues\n",
    "df['PTEBACC_94'] = df['PTEBACC_94'] * 100\n",
    "df['PTEBACC_95'] = df['PTEBACC_95'] * 100\n",
    "\n",
    "# calculate sqrt of the columns to create a more even distribution\n",
    "# sqrt selected over log to avoid zero value error issues\n",
    "for col in columns:\n",
    "    name = 'sqrt_' + col\n",
    "    df[name] = np.sqrt(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>No Feature Engineering</th>\n",
       "      <th>Feature Engineering 1st Pass</th>\n",
       "      <th>Feature Engineering 2nd Pass</th>\n",
       "      <th>Normalised Features</th>\n",
       "      <th>Uniform Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>k-NN</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Classifier  No Feature Engineering  Feature Engineering 1st Pass  \\\n",
       "0             Baseline                   0.592                         0.592   \n",
       "1  Logistic Regression                   0.627                         0.643   \n",
       "2                 k-NN                   0.191                         0.191   \n",
       "3        MultinomialNB                   0.417                         0.420   \n",
       "4           GaussianNB                   0.573                         0.573   \n",
       "5          BernoulliNB                   0.579                         0.571   \n",
       "\n",
       "   Feature Engineering 2nd Pass  Normalised Features  Uniform Features  \n",
       "0                         0.592                0.592             0.592  \n",
       "1                         0.647                0.665             0.667  \n",
       "2                         0.191                0.628             0.629  \n",
       "3                         0.435                0.604             0.606  \n",
       "4                         0.595                0.581             0.580  \n",
       "5                         0.579                0.600             0.600  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label the model run for the table\n",
    "label = 'Uniform Features'\n",
    "\n",
    "# define new feature columns\n",
    "X = df.drop(['LA', 'GENDER', 'RELCHAR', 'PPERSABS10', 'P8_BANDING', 'OVERALL_DESTPER', 'NOT_SUSTAINEDPER', 'UNKNOWNPER',\n",
    "       'OVERALL_DESTPER_DIS', 'NOT_SUSTAINEDPER_DIS', 'UNKNOWNPER_DIS', 'Rating', 'P8MEA', 'P8CILOW', 'P8CIUPP',\n",
    "            'PSENELK', 'PFSM', 'PNUMFSMEVER', 'PTEBACC_94', 'PTEBACC_95', 'PPERSABS10_NO_SUPP'], axis=1)\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "y = df['Rating']\n",
    "\n",
    "# run series of ML classifiers to assess accuracy\n",
    "acc = [baseline]\n",
    "\n",
    "for clf in classifiers:\n",
    "    scores = cross_val_score(clf, X, y, cv=5, scoring = 'accuracy')\n",
    "    acc.append(round(scores.mean(), 3))\n",
    "    \n",
    "# append new accuracy scores to table for comparison\n",
    "acc_scores[label] = acc\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifying the features to be more uniform appears to have minimal impact on the accuracy of the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>No Feature Engineering</th>\n",
       "      <th>Feature Engineering 1st Pass</th>\n",
       "      <th>Feature Engineering 2nd Pass</th>\n",
       "      <th>Normalised Features</th>\n",
       "      <th>Uniform Features</th>\n",
       "      <th>2nd Order Poly Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>k-NN</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Classifier  No Feature Engineering  Feature Engineering 1st Pass  \\\n",
       "0             Baseline                   0.592                         0.592   \n",
       "1  Logistic Regression                   0.627                         0.643   \n",
       "2                 k-NN                   0.191                         0.191   \n",
       "3        MultinomialNB                   0.417                         0.420   \n",
       "4           GaussianNB                   0.573                         0.573   \n",
       "5          BernoulliNB                   0.579                         0.571   \n",
       "\n",
       "   Feature Engineering 2nd Pass  Normalised Features  Uniform Features  \\\n",
       "0                         0.592                0.592             0.592   \n",
       "1                         0.647                0.665             0.667   \n",
       "2                         0.191                0.628             0.629   \n",
       "3                         0.435                0.604             0.606   \n",
       "4                         0.595                0.581             0.580   \n",
       "5                         0.579                0.600             0.600   \n",
       "\n",
       "   2nd Order Poly Features  \n",
       "0                    0.592  \n",
       "1                    0.670  \n",
       "2                    0.639  \n",
       "3                    0.555  \n",
       "4                    0.509  \n",
       "5                    0.303  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Creating polynomial features\n",
    "# 2nd order polynomial chosen as higher orders would take too long to process\n",
    "poly = preprocessing.PolynomialFeatures(degree = 2, interaction_only = False)\n",
    "\n",
    "# label the model run for the table\n",
    "label = '2nd Order Poly Features'\n",
    "\n",
    "# define new feature columns\n",
    "X = df.drop(['LA', 'GENDER', 'RELCHAR', 'PPERSABS10', 'P8_BANDING', 'OVERALL_DESTPER', 'NOT_SUSTAINEDPER', 'UNKNOWNPER',\n",
    "       'OVERALL_DESTPER_DIS', 'NOT_SUSTAINEDPER_DIS', 'UNKNOWNPER_DIS', 'Rating', 'P8MEA', 'P8CILOW', 'P8CIUPP',\n",
    "            'PSENELK', 'PFSM', 'PNUMFSMEVER', 'PTEBACC_94', 'PTEBACC_95', 'PPERSABS10_NO_SUPP'], axis=1)\n",
    "X = poly.fit_transform(X)\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "y = df['Rating']\n",
    "\n",
    "# run series of ML classifiers to assess accuracy\n",
    "acc = [baseline]\n",
    "\n",
    "for clf in classifiers:\n",
    "    scores = cross_val_score(clf, X, y, cv=5, scoring = 'accuracy')\n",
    "    acc.append(round(scores.mean(), 3))\n",
    "    \n",
    "# append new accuracy scores to table for comparison\n",
    "acc_scores[label] = acc\n",
    "acc_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating polynomial features in the dataset has provided the largest positive change for the k-NN classifier, which has seen an increase in accuracy of 1%. <br><br>\n",
    "The change has seen a large negative impact on all of the Naive Bayes classifiers, with the largest change seen in the Bernoulli method which has decreased in accuracy by 29.7%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Feature Selection Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is done to reduce noise from a dataset for machine learning purposes and is typically done using two main method types, filtering or wrapping. <br>\n",
    "\n",
    "Filter methods involve selection of the most influential features based on a property of the features themselves (such as variance in the feature or correlation to a target variable).  To implement the method the features are ranked based on the property of choice and the top x features selected based on assessment from the user.  The advantage of filter methods is speed as they are typically very easy to implement and do not take much processing power to run, allowing them to be efficiently applied to high dimensional datasets.  The disadvantage is that they do not relate directly to the end product of the classifier (e.g. accuracy or root-mean-squared-error), meaning that there is a possibility that the characteristic being filtered for does not actually improve the error from the machine learning model. <br>\n",
    "\n",
    "Wrapper methods on the other hand directly use the results of the machine learning model applied to reduce the number of chosen features from a dataset.  To implement the method a loop function is set up which runs the model with all the available features individually and chooses the one that reduces the error by the largest factor, before repeating this loop by cycling through all the remaining features in combination with the first and selecting the one which reduces the model error by the most.  This loop cycle is then repeated until the chosen number of features defined by the operator is selected.  The advantage of this technique is its intrinsic reduction in the error of the machine learning model which is absent from the filter method.  The disadvantage however is that the technique is inherently slow and requires a lot of processing power, meaning it is very time and resource costly to implement for high dimensional datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Filter Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 potential filter method techniques for us to use on the dataset are as follows:\n",
    "\n",
    "1. Filtering features based on variance\n",
    "    - variance indicates the spread of the data in a feature, the higher the variance the more the data is spread out allowing a model a wider range of values to utilise when predicting the target class\n",
    "    - for our dataset we can rank the features by their variance and select the number of features which shows the highest accuracy score <br><br>\n",
    "2. Filtering based on correlation to the target class\n",
    "    - correlation indicates the strength of a statistical relationship between two variables, the more a feature is correlated to the target the more useful it will be to predict that target class\n",
    "    - for our data set we can use the F-test statistic (as the features are mostly continuous and the target is categorical) to rank the features for their correlation to the target class and select the number of features which shows the highest accuracy score<br><br>\n",
    "3. Removing mutual information\n",
    "    - some features in a dataset are directly calculated from others or can be predicted directly from them, when this is the case having both features adds no new information to the data and simply increases noise for the ML algorithm and they can be removed\n",
    "    - for our dataset we can remove complementary data features (such as the 'PNORG' and 'PNORB' which always sum to 1)   \n",
    "    \n",
    "For our data we will use method 2, ranking the features based on their correlation to the target class.  Note that the 2nd order polynomial transform has been removed from the data to do this as leaving it in caused several error messages on the algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>No Feature Engineering</th>\n",
       "      <th>Feature Engineering 1st Pass</th>\n",
       "      <th>Feature Engineering 2nd Pass</th>\n",
       "      <th>Normalised Features</th>\n",
       "      <th>Uniform Features</th>\n",
       "      <th>2nd Order Poly Features</th>\n",
       "      <th>Filter Method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>k-NN</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Classifier  No Feature Engineering  Feature Engineering 1st Pass  \\\n",
       "0             Baseline                   0.592                         0.592   \n",
       "1  Logistic Regression                   0.627                         0.643   \n",
       "2                 k-NN                   0.191                         0.191   \n",
       "3        MultinomialNB                   0.417                         0.420   \n",
       "4           GaussianNB                   0.573                         0.573   \n",
       "5          BernoulliNB                   0.579                         0.571   \n",
       "\n",
       "   Feature Engineering 2nd Pass  Normalised Features  Uniform Features  \\\n",
       "0                         0.592                0.592             0.592   \n",
       "1                         0.647                0.665             0.667   \n",
       "2                         0.191                0.628             0.629   \n",
       "3                         0.435                0.604             0.606   \n",
       "4                         0.595                0.581             0.580   \n",
       "5                         0.579                0.600             0.600   \n",
       "\n",
       "   2nd Order Poly Features  Filter Method  \n",
       "0                    0.592          0.592  \n",
       "1                    0.670          0.669  \n",
       "2                    0.639          0.639  \n",
       "3                    0.555          0.607  \n",
       "4                    0.509          0.626  \n",
       "5                    0.303          0.629  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label the model run for the table\n",
    "label = 'Filter Method'\n",
    "\n",
    "# ANOVA feature selection for numeric input and categorical output\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "X = df.drop(['LA', 'GENDER', 'RELCHAR', 'PPERSABS10', 'P8_BANDING', 'OVERALL_DESTPER', 'NOT_SUSTAINEDPER', 'UNKNOWNPER',\n",
    "       'OVERALL_DESTPER_DIS', 'NOT_SUSTAINEDPER_DIS', 'UNKNOWNPER_DIS', 'Rating', 'P8MEA', 'P8CILOW', 'P8CIUPP',\n",
    "            'PSENELK', 'PFSM', 'PNUMFSMEVER', 'PTEBACC_94', 'PTEBACC_95', 'PPERSABS10_NO_SUPP'], axis=1)\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "y = df['Rating']\n",
    "\n",
    "acc = [baseline]\n",
    "\n",
    "for clf in classifiers:\n",
    "    # run filter loop to look through all possible numbers of features to use\n",
    "    clf_acc = []\n",
    "    for x in range(1, 28):\n",
    "        # define feature selection\n",
    "        fs = SelectKBest(score_func=f_classif, k=x)\n",
    "        # apply feature selection\n",
    "        X_selected = fs.fit_transform(X, y)\n",
    "        # calculate the cross validated accuracy\n",
    "        scores = cross_val_score(clf, X_selected, y, cv=5, scoring = 'accuracy')\n",
    "        clf_acc.append(round(scores.mean(),3))\n",
    "        clf_acc_max = max(clf_acc)\n",
    "    acc.append(clf_acc_max)\n",
    "\n",
    "# update table\n",
    "acc_scores[label] = acc\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application of the filter method has seen the largest gains for the GaussianNB and BernoulliNB classifiers, with increases in accuracy of 4.6 and 2.9% respectively. <br><br>\n",
    "The k-NN classifier also showed a small increase in accuracy from this feature selection technique showing a gain in accuracy of 1% similar to that seen from generating 2nd order polynomial features in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Wrapper Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-NN means classifier was chosen for the wrapper method as this had shown the most improvement during the feature selection so far.  Again the polynomial feature transformation has been dropped as it produced error warnings with the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>No Feature Engineering</th>\n",
       "      <th>Feature Engineering 1st Pass</th>\n",
       "      <th>Feature Engineering 2nd Pass</th>\n",
       "      <th>Normalised Features</th>\n",
       "      <th>Uniform Features</th>\n",
       "      <th>2nd Order Poly Features</th>\n",
       "      <th>Filter Method</th>\n",
       "      <th>Wrapper Method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.669</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>k-NN</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.607</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.626</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.629</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Classifier  No Feature Engineering  Feature Engineering 1st Pass  \\\n",
       "0             Baseline                   0.592                         0.592   \n",
       "1  Logistic Regression                   0.627                         0.643   \n",
       "2                 k-NN                   0.191                         0.191   \n",
       "3        MultinomialNB                   0.417                         0.420   \n",
       "4           GaussianNB                   0.573                         0.573   \n",
       "5          BernoulliNB                   0.579                         0.571   \n",
       "\n",
       "   Feature Engineering 2nd Pass  Normalised Features  Uniform Features  \\\n",
       "0                         0.592                0.592             0.592   \n",
       "1                         0.647                0.665             0.667   \n",
       "2                         0.191                0.628             0.629   \n",
       "3                         0.435                0.604             0.606   \n",
       "4                         0.595                0.581             0.580   \n",
       "5                         0.579                0.600             0.600   \n",
       "\n",
       "   2nd Order Poly Features  Filter Method  Wrapper Method  \n",
       "0                    0.592          0.592           0.592  \n",
       "1                    0.670          0.669             NaN  \n",
       "2                    0.639          0.639           0.658  \n",
       "3                    0.555          0.607             NaN  \n",
       "4                    0.509          0.626             NaN  \n",
       "5                    0.303          0.629             NaN  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label the model run for the table\n",
    "label = 'Wrapper Method'\n",
    "\n",
    "# wrapper method used to define the most influencial features from the dataset\n",
    "# run in a loop to see the effects of adding more features into the classifier\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "sfs = SFS(neighbors.KNeighborsClassifier(), k_features=(1, 28), forward=True, floating=False, scoring = 'accuracy', cv = 5)\n",
    "sfs.fit(X, y)\n",
    "\n",
    "# ammend table with wrapper method results\n",
    "acc = [np.nan] * 6\n",
    "acc_scores[label] = acc\n",
    "acc_scores.loc[acc_scores['Classifier'] == 'Baseline', 'Wrapper Method'] = baseline\n",
    "acc_scores.loc[acc_scores['Classifier'] == 'k-NN', 'Wrapper Method'] = round(pd.DataFrame(sfs.get_metric_dict()).loc['avg_score', :].max(),3)\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrapper feature selection method has provided an improvement of 2.9% in accuracy for the k-NN classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
